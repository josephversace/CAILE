Project Overview
Intelligent Investigation Machine (IIM) - A Blazor Hybrid + WSL2 desktop application for law enforcement AI investigations. Premium training platform delivered as part of the "AI Investigator Mastery Bootcamp" running entirely on-premises on Framework PCs with 128GB RAM and AMD GPUs.

1. System Architecture Overview
mermaidgraph TB
    subgraph "Windows Host"
        subgraph "IIM.Desktop (WinForms)"
            MW[MainForm + WebView2]
            MW --> BC[Blazor Components]
        end
        
        subgraph "IIM.Api (localhost:5080)"
            API[Minimal APIs]
            API --> MW2[Mediator]
            MW2 --> HSV[Hosted Services]
        end
        
        subgraph "Core Services"
            SK[SemanticKernel<br/>Orchestrator]
            IP[Inference<br/>Pipeline]
            MO[Model<br/>Orchestrator]
            SK --> IP
            IP --> MO
        end
        
        subgraph "Storage"
            SC[Storage<br/>Configuration]
            ES[Evidence<br/>Store]
            MT[Model<br/>Templates]
        end
    end
    
    subgraph "WSL2 Ubuntu"
        QD[Qdrant<br/>:6333]
        EMB[Embedding<br/>Service :8081]
        PG[PostgreSQL<br/>:5432]
        MIN[MinIO<br/>:9000]
    end
    
    BC -.->|HTTP| API
    API <-->|HTTP| QD
    API <-->|HTTP| EMB
    MO --> SC
    ES --> SC
2. Investigation Query Flow (Current State)
mermaidsequenceDiagram
    participant UI as Blazor UI
    participant API as API Layer
    participant Med as Mediator
    participant SK as Semantic Kernel
    participant IP as Inference Pipeline
    participant MO as Model Orchestrator
    participant WSL as WSL Services
    
    Note over UI,WSL: ‚ö†Ô∏è Current: Partially Connected
    
    UI->>API: POST /api/investigation/query
    API->>Med: Send(ProcessQueryCommand)
    Med->>SK: ProcessQueryAsync()
    
    Note over SK: Currently just simulates<br/>with Task.Delay()
    SK->>SK: ExtractIntent() [STUB]
    SK->>SK: BuildActionPlan() [STUB]
    
    SK->>IP: ExecuteAsync<T>()
    Note over IP: Queues request but<br/>no actual inference
    IP->>MO: InferAsync()
    Note over MO: Would load model<br/>but not connected
    
    MO-->>IP: [Mock Result]
    IP-->>SK: [Mock Result]
    SK-->>Med: ReasoningResult
    Med-->>API: Response
    API-->>UI: JSON Response
    
    Note over WSL: ‚ùå Not Connected:<br/>Qdrant, Embeddings
3. Model Loading Flow (Current State)
mermaidflowchart LR
    subgraph "Current Implementation"
        TC[Template<br/>Configuration] --> TL[Template<br/>Loader]
        TL --> MC[Memory<br/>Check]
        MC --> MO2[Model<br/>Orchestrator]
        MO2 --> FS[File<br/>System]
        FS --> OM[ONNX<br/>Model]
    end
    
    subgraph "Missing Connections"
        MO2 -.->|‚ùå| IP2[Inference<br/>Pipeline]
        IP2 -.->|‚ùå| AR[Actual<br/>Results]
    end
4. Evidence Ingestion Flow (Intended vs Current)
mermaidgraph TD
    subgraph "Intended Flow"
        F1[File Upload] --> H1[SHA-256 Hash]
        H1 --> E1[Text Extraction]
        E1 --> EM1[Embedding Service]
        EM1 --> V1[Vector Store]
        V1 --> Q1[Qdrant Index]
    end
    
    subgraph "Current State"
        F2[File Upload] --> H2[SHA-256 Hash]
        H2 --> E2[Text Extraction]
        E2 -.->|‚ùå| EM2[Embedding Service]
        EM2 -.->|‚ùå| V2[Vector Store]
        V2 -.->|‚ùå| Q2[Qdrant Index]
    end
    
    style EM2 stroke:#f66,stroke-width:2px
    style V2 stroke:#f66,stroke-width:2px
    style Q2 stroke:#f66,stroke-width:2px
5. Mediator Communication Pattern
mermaidgraph TB
    subgraph "Commands (State Changes)"
        C1[LoadModelCommand]
        C2[CreateSessionCommand]
        C3[ConfigureProxyCommand]
    end
    
    subgraph "Queries (Read Data)"
        Q1[GetWslStatusQuery]
        Q2[GetAvailableModelsQuery]
        Q3[GetSessionQuery]
    end
    
    subgraph "Notifications (Events)"
        N1[ModelLoadedNotification]
        N2[InferenceCompletedNotification]
        N3[WslSetupCompletedNotification]
    end
    
    subgraph "Mediator Pipeline"
        M[Mediator]
        M --> VB[Validation]
        VB --> AB[Authorization]
        AB --> LB[Logging]
        LB --> PB[Performance]
        PB --> CB[Caching]
        CB --> H[Handler]
    end
    
    C1 --> M
    Q1 --> M
    H --> N1
    
    subgraph "Multiple Handlers"
        N1 --> H1[UI Update Handler]
        N1 --> H2[Audit Log Handler]
        N1 --> H3[Metrics Handler]
    end
6. WSL Service Management Flow
mermaidstateDiagram-v2
    [*] --> CheckWSL: App Starts
    
    CheckWSL --> WSLNotInstalled: Not Found
    CheckWSL --> WSLInstalled: Found
    
    WSLNotInstalled --> ShowSetupUI: Guide User
    ShowSetupUI --> GenerateScript: Create PS Script
    GenerateScript --> UserRunsScript: Manual Action
    UserRunsScript --> WSLInstalled: After Restart
    
    WSLInstalled --> CheckDistro: Check Ubuntu
    CheckDistro --> DistroMissing: Not Found
    CheckDistro --> DistroReady: Found
    
    DistroMissing --> InstallDistro: Auto Install
    InstallDistro --> DistroReady: Success
    
    DistroReady --> StartServices: Start Stack
    StartServices --> CheckHealth: Monitor
    
    CheckHealth --> ServicesHealthy: All Good
    CheckHealth --> ServicesFailed: Issues
    ServicesFailed --> RestartServices: Auto Retry
    RestartServices --> CheckHealth: Re-check
    
    ServicesHealthy --> [*]: Ready
7. Inference Pipeline Queue System
mermaidgraph LR
    subgraph "Request Entry"
        R1[High Priority]
        R2[Normal Priority]
        R3[Low Priority]
    end
    
    subgraph "Queue Channels"
        Q1[High Queue<br/>Channel]
        Q2[Normal Queue<br/>Channel]
        Q3[Low Queue<br/>Channel]
    end
    
    subgraph "Processing"
        P1[GPU Semaphore<br/>Max: 2]
        P2[CPU Semaphore<br/>Max: 4]
        PT[Processing<br/>Tasks]
    end
    
    subgraph "Current Issue"
        PT -.->|‚ùå| EX[Actual<br/>Execution]
        EX -.->|Missing| RES[Real<br/>Results]
    end
    
    R1 --> Q1
    R2 --> Q2
    R3 --> Q3
    
    Q1 --> PT
    Q2 --> PT
    Q3 --> PT
    
    PT --> P1
    PT --> P2
8. Complete Data Flow - Investigation Query (What Should Happen)
mermaidgraph TD
    subgraph "1. Query Input"
        U[User Query] --> UI[Blazor UI]
    end
    
    subgraph "2. Command Layer"
        UI --> CMD[ProcessQueryCommand]
        CMD --> MED[Mediator]
    end
    
    subgraph "3. Intelligence Layer"
        MED --> SK[Semantic Kernel]
        SK --> IE[Intent Extraction]
        IE --> AP[Action Planning]
        AP --> RT[Route to Tools/Models]
    end
    
    subgraph "4. Execution Layer"
        RT --> IP[Inference Pipeline]
        IP --> QM[Queue Management]
        QM --> MO[Model Orchestrator]
    end
    
    subgraph "5. Infrastructure Layer"
        MO --> LM[Load Models]
        RT --> EMB[Embedding Service]
        EMB --> QD[Qdrant Search]
        QD --> CTX[Context Retrieval]
    end
    
    subgraph "6. Generation"
        CTX --> LLM[LLM Generation]
        LLM --> CIT[Add Citations]
    end
    
    subgraph "7. Response"
        CIT --> RESP[Response]
        RESP --> NOT[Notifications]
        RESP --> UI2[Update UI]
    end
    
    style EMB stroke:#f66,stroke-width:2px
    style QD stroke:#f66,stroke-width:2px
    style LLM stroke:#f66,stroke-width:2px
9. Service Dependency Map
mermaidgraph BT
    subgraph "Infrastructure Layer"
        WSL[WslManager]
        STOR[StorageConfiguration]
        HTTP[HttpClientFactory]
    end
    
    subgraph "Core Services"
        MO[ModelOrchestrator]
        IP[InferencePipeline]
        SK[SemanticKernelOrchestrator]
    end
    
    subgraph "Application Services"
        IS[InvestigationService]
        SS[SessionService]
        MS[ModelManagementService]
    end
    
    subgraph "UI Components"
        INV[Investigation.razor]
        WSLUI[WslStatus.razor]
        MOD[Models.razor]
    end
    
    MO --> STOR
    IP --> MO
    SK --> MO
    SK --> SS
    
    IS --> SK
    IS --> SS
    MS --> MO
    
    INV --> IS
    WSLUI --> WSL
    MOD --> MS
    
    style SK fill:#ffa
    style IP fill:#ffa
    Note1[üü° Partially Implemented]

Key Observations
‚úÖ What's Connected:

UI ‚Üî API communication
Mediator pattern infrastructure
Model template system
Basic file operations

‚ö†Ô∏è Partially Connected:

Semantic Kernel (structure only)
Inference Pipeline (queuing only)
WSL Manager (not auto-starting)

‚ùå Not Connected:

WSL services (Qdrant, Embeddings)
Actual model inference execution
RAG pipeline
Real-time notifications to UI
Tool implementations (OCR, OSINT, etc.)


Current State (What's Actually Working)
‚úÖ Implemented & Functional:

Desktop Shell: Windows Forms + Blazor WebView UI rendering
Storage Configuration: Path management for models/evidence/templates
Model Templates: Complete structure for investigation types (CSAM, Financial, etc.)
Queue System: Priority-based InferencePipeline with GPU/CPU resource management
WSL Manager: Core WSL operations (install, enable, execute commands)
Mediator Infrastructure: MediatR setup with pipeline behaviors
Basic Models: DefaultModelOrchestrator can load/unload ONNX models

‚ö†Ô∏è Partially Implemented:

SemanticKernelOrchestrator: Structure exists but doesn't route to actual execution (just simulates with delays)
InferencePipeline: Queue management works but missing actual inference execution
Investigation Service: Framework exists but tools return mock data
Notification System: Publishing events but no UI subscription

‚ùå Not Connected/Missing:

WSL Auto-start: Commented out in TrayService
Vector Store: No Qdrant client implementation
Embedding Service: No connection to FastAPI service
Tool Implementations: OCR, OSINT, Redaction are stubs
Command/Query Handlers: Many defined but not implemented
Real-time Updates: No SignalR/WebSocket connection

Intended State (The Vision)
A local-first AI investigation platform where:

User opens app ‚Üí WSL services auto-start (Qdrant, embeddings, etc.)
Selects investigation template ‚Üí Models load based on available memory
Drags in evidence ‚Üí Files are hashed, extracted, embedded, indexed
Asks questions ‚Üí SK routes through appropriate models/tools/RAG
Gets grounded answers ‚Üí With citations, evidence links, and audit trail
All offline ‚Üí No cloud dependencies, complete privacy

System Overview
User Interface (Blazor)
    ‚Üì [Commands/Queries via Mediator]
Semantic Kernel (Intelligence - WHAT to do)
    ‚Üì [Determines required operations]
Inference Pipeline (Scheduling - WHEN to run)  
    ‚Üì [Manages queue/priority]
Model Orchestrator (Resources - HOW to load)
    ‚Üì [Manages memory/models]
WSL Services (Infrastructure)
    - Qdrant (Vector DB)
    - Embedding Service (FastAPI)
    - Future: Ollama, etc.
The Plan - Next Sprint Priorities
Sprint 1: Foundation (Week 1)
Goal: Get WSL services running and connected

Fix WSL Auto-start
csharp// Uncomment in TrayService
public void EnsureStarted() { 
    if(!_wsl.IsWslEnabled()) _wsl.EnableWsl(); 
    _wsl.StartIimStackAsync(); 
}

Implement Qdrant Client
csharppublic interface IVectorStore {
    Task<string> IndexAsync(float[] embedding, Dictionary<string, object> metadata);
    Task<SearchResult[]> SearchAsync(float[] query, int topK);
}

Connect Embedding Service
csharppublic interface IEmbeddingService {
    Task<float[]> EmbedTextAsync(string text);
    Task<float[]> EmbedImageAsync(byte[] image);
}


Sprint 2: Execution Bridge (Week 2)
Goal: Connect SK orchestration to actual model execution

Wire up SemanticKernelOrchestrator.ExecuteStepAsync()

Route to actual InferencePipeline
Connect to tool implementations
Return real results


Complete InferencePipeline execution

Add actual model inference call
Implement IModelMetadataService
Connect retry/error handling


Implement core tools

OCR (Tesseract integration)
Entity extraction (NER)
Basic OSINT (web search)



Sprint 3: Investigation Flow (Week 3)
Goal: Complete end-to-end investigation capability

Evidence ingestion pipeline

File ‚Üí Hash ‚Üí Extract ‚Üí Embed ‚Üí Index


RAG implementation

Query ‚Üí Embed ‚Üí Search ‚Üí Retrieve ‚Üí Generate


Template-based model loading

Memory checking
User selection for constrained memory
Progress reporting



Sprint 4: Polish & Monitoring (Week 4)
Goal: Production readiness

Health monitoring

Service health checks
Auto-restart failed services
Status dashboard


Real-time updates

SignalR for notifications
Progress indicators
Live logs


Error handling

Graceful degradation
User-friendly error messages
Recovery procedures



Success Metrics

Week 1: Can start WSL, index a document, search vectors
Week 2: Can load a model and get real inference results
Week 3: Can complete a full investigation query with RAG
Week 4: Stable, monitored, production-ready system

Risk Mitigations

Memory constraints: Implement swap/offloading for large models
WSL issues: Provide manual setup scripts as fallback
Performance: Add caching layer for repeated queries
Debugging: Comprehensive logging at each integration point