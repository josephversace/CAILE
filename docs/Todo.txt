# IIM Platform Refactoring - Continuation Prompt (Updated December 2024)

## Project Overview
You are working on the **Intelligent Investigation Machine (IIM)** - a Blazor Hybrid + WSL2 desktop application for law enforcement AI investigations. This is a premium training platform delivered as part of the "AI Investigator Mastery Bootcamp" that runs entirely on-premises on Framework PCs with 128GB RAM and AMD GPUs (Strix hardware).

## Business Context
- **4-Day Training Course**: Includes Framework Desktop + IIM Platform
- **Certification**: Certified AI Investigator â€“ Law Enforcement (CAI-LE)
- **Target Users**: Law enforcement investigators with no programming experience
- **Delivery Model**: Premium hardware + software package with member portal access

## Core Functionality
- **Local AI inference** using DirectML (primary) and Vulkan via LlamaSharp (secondary)
- **No ROCm on Windows** (too unstable, DirectML preferred for AMD GPUs)
- **RAG (Retrieval-Augmented Generation)** for evidence analysis with citations
- **Multi-modal support**: text (LLMs), image (CLIP), audio (Whisper), video analysis
- **Evidence management** with chain of custody and SHA-256 hashing
- **WSL2 integration** running Docker containers (PostgreSQL, Qdrant, MinIO)
- **Model Templates** for different investigation types (Fast, Deep Analysis, Multi-Modal)
- **Mediator pattern** for all operations with automatic logging, validation, caching, audit
- **Tor proxy support** for OSINT investigations (via new WslManager method)

## Technology Stack
- **.NET 8** with Blazor Hybrid (WebView2)
- **Windows Forms** host application (net8.0-windows10.0.17763.0)
- **Minimal APIs** for backend (no MVC Controllers)
- **WSL2** with Docker for services:
  - Ubuntu distro for services (Qdrant, PostgreSQL, MinIO)
  - Kali distro for forensic tools (MCP server integration)
- **DirectML** for GPU acceleration (AMD Strix optimized)
- **Custom Mediator** implementation (no MediatR dependency)
- **Pipeline behaviors** for cross-cutting concerns (7 implemented)

## Current Solution Structure
```
/src
â”œâ”€â”€ IIM.Shared/          # DTOs, Enums, Common Models (no dependencies)
â”‚   â”œâ”€â”€ DTOs/           # API request/response contracts
â”‚   â”œâ”€â”€ Enums/          # Shared enumerations  
â”‚   â””â”€â”€ Models/         # TimeRange, FileMetadata, etc.
â”‚
â”œâ”€â”€ IIM.Infrastructure/  # External services, WSL, Storage
â”‚   â””â”€â”€ Platform/       # âœ… PHASE 1 COMPLETE
â”‚       â”œâ”€â”€ IWslManager.cs (interface with InstallTorAndApplyProxyAsync)
â”‚       â”œâ”€â”€ WslManager.cs (monolithic implementation - needs refactoring)
â”‚       â””â”€â”€ WslServiceOrchestrator.cs
â”‚
â”œâ”€â”€ IIM.Plugin.SDK/     # Plugin interfaces
â”‚
â”œâ”€â”€ IIM.Core/           # Domain models, business logic
â”‚   â”œâ”€â”€ Models/         # Case, Evidence, Investigation, Inference models
â”‚   â”œâ”€â”€ Services/       # Core business services (many partially implemented)
â”‚   â”œâ”€â”€ AI/            
â”‚   â”‚   â”œâ”€â”€ ModelOrchestrator.cs (âš ï¸ Partial - needs DirectML integration)
â”‚   â”‚   â””â”€â”€ DefaultModelOrchestrator.cs (has InferAsync stub)
â”‚   â”œâ”€â”€ Inference/     
â”‚   â”‚   â””â”€â”€ InferencePipeline.cs (âš ï¸ NEXT PRIORITY - needs completion)
â”‚   â”œâ”€â”€ RAG/           # âš ï¸ Empty implementations
â”‚   â””â”€â”€ Mediator/      # âœ… Custom mediator (complete)
â”‚
â”œâ”€â”€ IIM.Application/    # Application services
â”‚   â”œâ”€â”€ Commands/      # âœ… CQRS commands with handlers
â”‚   â”‚   â”œâ”€â”€ Wsl/       # Including new ConfigureProxyCommand
â”‚   â”‚   â”œâ”€â”€ Models/    # Load/Unload model commands
â”‚   â”‚   â””â”€â”€ Investigation/
â”‚   â”œâ”€â”€ Queries/       # âœ… Query handlers
â”‚   â”œâ”€â”€ Behaviors/     # âœ… 7 Pipeline behaviors implemented
â”‚   â””â”€â”€ Services/      
â”‚       â”œâ”€â”€ InferenceService.cs (âš ï¸ Needs DirectML implementation)
â”‚       â””â”€â”€ InvestigationService.cs (partial implementation)
â”‚
â”œâ”€â”€ IIM.Api/           # HTTP endpoints, Minimal APIs
â”‚   â””â”€â”€ Program.cs     # All endpoints defined here
â”‚
â”œâ”€â”€ IIM.Components/    # Blazor UI components
â”‚   â””â”€â”€ Components/
â”‚       â””â”€â”€ Settings/
â”‚           â””â”€â”€ ProxySetup.razor (âœ… New Tor proxy UI)
â”‚
â””â”€â”€ IIM.Desktop/       # Windows Forms host
    â””â”€â”€ Program.cs     # Mediator configuration, DI setup

/tests
â”œâ”€â”€ IIM.Core.Tests/
â”‚   â”œâ”€â”€ Infrastructure/
â”‚   â”‚   â””â”€â”€ WslManagerTests.cs (exists but incomplete)
â”‚   â””â”€â”€ Mocks/
â”‚       â”œâ”€â”€ MockWslManager.cs
â”‚       â””â”€â”€ MockModelOrchestrator.cs
â””â”€â”€ IIM.Integration.Tests/
```

## âœ… Phase 1: WSL & Docker Foundation (COMPLETE)

### What Was Implemented:
1. **WslManager with core functionality**:
   - `GetStatusAsync()` - Check WSL status
   - `IsWslEnabled()` - Simple boolean check
   - `EnableWsl()` - Install WSL with admin/non-admin paths
   - `EnsureDistroAsync()` - Install Ubuntu/Kali distros
   - `StartIim()` - Start all IIM services
   - `StartServicesAsync()` - Start Docker containers
   - `GetNetworkInfoAsync()` - Get WSL network details
   - `HealthCheckAsync()` - Comprehensive health monitoring
   - `SyncFilesAsync()` - Windows to WSL file sync
   - `InstallDistroAsync()` - Install from tar.gz
   - **NEW**: `InstallTorAndApplyProxyAsync()` - Tor proxy setup

2. **Service Orchestration**:
   - Qdrant on port 6333 (vector database)
   - PostgreSQL on port 5432 (case management)
   - MinIO on ports 9000/9001 (evidence storage)
   - MCP server on port 3000 (Kali forensic tools)

3. **Non-admin support**:
   - Generates PowerShell scripts for IT
   - Desktop instructions for manual install
   - Fallback to portable mode

### Current Issues with Phase 1:
- âš ï¸ **WslManager is monolithic** (3000+ lines in one file)
- âš ï¸ **Models not properly separated** (inline classes in WslManager.cs)
- âš ï¸ **Tests incomplete** (MockWslManager throws NotImplementedException)
- âš ï¸ **No actual DirectML implementation** despite being primary GPU strategy

## ğŸ¯ Phase 2: Inference Pipeline & DirectML (CURRENT PRIORITY)

### Goal: Complete the AI inference pipeline with DirectML support

### Key Components Needing Implementation:

#### 1. **InferencePipeline.cs** 
Current state: Has structure but incomplete implementation
```csharp
// Needs:
- DirectML device initialization
- Proper queue processing
- Batch inference support
- Memory management for large models
```

#### 2. **ModelOrchestrator.cs**
Current state: Stub implementation, missing actual model loading
```csharp
// Needs:
- ONNX Runtime with DirectML provider
- Model download from member portal
- Model caching and versioning
- GPU memory management
```

#### 3. **InferenceService.cs**
Current state: Interface defined, implementation incomplete
```csharp
// Needs:
- Integration with InferencePipeline
- Support for all model types (LLM, Whisper, CLIP)
- Streaming responses for real-time transcription
```

### DirectML Integration Requirements:
1. **Device Enumeration**: Detect AMD GPU capabilities
2. **Model Optimization**: Convert models to ONNX format
3. **Memory Management**: Handle 128GB RAM + GPU VRAM
4. **Fallback Strategy**: CPU inference when GPU unavailable

## ğŸ“‹ Implementation Priorities

### Sprint 1: DirectML Foundation (Week 1)
```csharp
// 1. Create DirectML device manager
public class DirectMLDeviceManager
{
    - EnumerateDevices()
    - CreateDevice(int deviceId)
    - GetDeviceCapabilities()
    - EstimateMemoryRequirements(model)
}

// 2. Integrate with ModelOrchestrator
- Add DirectML execution provider to ONNX Runtime
- Implement model warmup/optimization
- Add performance monitoring
```

### Sprint 2: Complete InferencePipeline (Week 2)
```csharp
// 1. Fix queue processing
- Implement priority-based scheduling
- Add batch processing for efficiency
- Handle cancellation properly

// 2. Add model-specific handlers
- LLM streaming with LlamaSharp
- Whisper with chunk processing
- CLIP with batch image encoding
```

### Sprint 3: RAG Implementation (Week 3)
```csharp
// 1. Document ingestion pipeline
- PDF/DOCX parsing with proper chunking
- Embedding generation via ONNX
- Qdrant integration for vector storage

// 2. Retrieval and generation
- Semantic search with reranking
- Context window management
- Citation extraction and formatting
```

### Sprint 4: Integration & Testing (Week 4)
```csharp
// 1. Wire up Mediator commands
- ProcessInvestigationCommand â†’ InferencePipeline
- LoadModelCommand â†’ ModelOrchestrator
- SearchDocumentsQuery â†’ RAG pipeline

// 2. End-to-end testing
- Full investigation workflow
- Performance benchmarks
- Memory leak detection
```

## ğŸ”§ Refactoring Needs

### Immediate (Before Phase 2):
1. **Break up WslManager.cs**:
   - WslManager.Core.cs (main logic)
   - WslManager.Services.cs (Docker management)
   - WslManager.Network.cs (networking)
   - WslManager.Proxy.cs (Tor/proxy setup)
   - Move models to separate files

2. **Fix Mock implementations**:
   - Complete MockWslManager
   - Add MockInferencePipeline
   - Enable unit testing

### During Phase 2:
1. **Standardize error handling** with Result<T> pattern
2. **Add telemetry** for performance monitoring
3. **Implement caching** for model outputs
4. **Add progress reporting** for long operations

## ğŸ§ª Testing Requirements

### Unit Tests Needed:
- [ ] DirectMLDeviceManager device enumeration
- [ ] InferencePipeline queue management
- [ ] ModelOrchestrator load/unload cycles
- [ ] RAG chunking strategies
- [ ] Mediator command handlers

### Integration Tests:
- [ ] WSL + Docker service startup
- [ ] Model download and caching
- [ ] End-to-end inference pipeline
- [ ] Multi-modal investigation workflow

## ğŸ“Š Success Metrics

Phase 2 is complete when:
- âœ… DirectML successfully initialized on AMD GPU
- âœ… At least one model of each type loads (LLM, Whisper, CLIP)
- âœ… Inference pipeline processes requests with <500ms overhead
- âœ… RAG returns relevant results with citations
- âœ… Memory usage stays under 64GB for typical workload
- âœ… All mock implementations replaced
- âœ… 80% test coverage on critical paths

## ğŸ’¡ Technical Decisions

1. **DirectML over ROCm**: ROCm unstable on Windows, DirectML is Microsoft-supported
2. **ONNX as primary format**: Best DirectML compatibility
3. **LlamaSharp for LLMs**: Native C# implementation, good Vulkan support
4. **Custom Mediator**: Avoid external dependencies, full control
5. **Monolithic to start**: Get working, then refactor (tech debt acknowledged)

## ğŸš€ Quick Start for Phase 2

```csharp
// 1. Initialize DirectML
var deviceManager = new DirectMLDeviceManager(logger);
var device = await deviceManager.CreateDevice(0); // Primary GPU

// 2. Load model with DirectML
var orchestrator = new ModelOrchestrator(logger, deviceManager);
await orchestrator.LoadModelAsync(new ModelRequest 
{
    ModelId = "llama-3.2-3b-onnx",
    Provider = "DirectML",
    DeviceId = 0
});

// 3. Run inference
var pipeline = new InferencePipeline(logger, orchestrator);
var result = await pipeline.ExecuteAsync<string>(new InferencePipelineRequest
{
    ModelId = "llama-3.2-3b-onnx",
    Input = "Analyze this evidence...",
    Priority = 2 // High priority
});
```

## ğŸ“ Notes for Developer

- **Current state**: WSL management works, AI inference is mostly stubs
- **Next priority**: Get DirectML working with real models
- **Blockers**: No actual model files yet (need portal API integration)
- **Tech debt**: WslManager needs refactoring but works
- **Testing**: Mocks exist but many throw NotImplementedException

Remember: This is for a law enforcement training course - reliability and ease of use are paramount. The students are not developers, so the system must "just work" out of the box.