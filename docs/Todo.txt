# IIM Platform Refactoring - Continuation Prompt (Updated December 2024)

## Project Overview
You are working on the **Intelligent Investigation Machine (IIM)** - a Blazor Hybrid + WSL2 desktop application for law enforcement AI investigations. This is a premium training platform delivered as part of the "AI Investigator Mastery Bootcamp" that runs entirely on-premises on Framework PCs with 128GB RAM and AMD GPUs (Strix hardware).

## Business Context
- **4-Day Training Course**: Includes Framework Desktop + IIM Platform
- **Certification**: Certified AI Investigator – Law Enforcement (CAI-LE)
- **Target Users**: Law enforcement investigators with no programming experience
- **Delivery Model**: Premium hardware + software package with member portal access

## Core Functionality
- **Local AI inference** using DirectML (primary) and Vulkan via LlamaSharp (secondary)
- **No ROCm on Windows** (too unstable, DirectML preferred for AMD GPUs)
- **RAG (Retrieval-Augmented Generation)** for evidence analysis with citations
- **Multi-modal support**: text (LLMs), image (CLIP), audio (Whisper), video analysis
- **Evidence management** with chain of custody and SHA-256 hashing
- **WSL2 integration** running Docker containers (PostgreSQL, Qdrant, MinIO)
- **Model Templates** for different investigation types (Fast, Deep Analysis, Multi-Modal)
- **Mediator pattern** for all operations with automatic logging, validation, caching, audit
- **Tor proxy support** for OSINT investigations (via new WslManager method)

## Technology Stack
- **.NET 8** with Blazor Hybrid (WebView2)
- **Windows Forms** host application (net8.0-windows10.0.17763.0)
- **Minimal APIs** for backend (no MVC Controllers)
- **WSL2** with Docker for services:
  - Ubuntu distro for services (Qdrant, PostgreSQL, MinIO)
  - Kali distro for forensic tools (MCP server integration)
- **DirectML** for GPU acceleration (AMD Strix optimized)
- **Custom Mediator** implementation (no MediatR dependency)
- **Pipeline behaviors** for cross-cutting concerns (7 implemented)

## Current Solution Structure
```
/src
├── IIM.Shared/          # DTOs, Enums, Common Models (no dependencies)
│   ├── DTOs/           # API request/response contracts
│   ├── Enums/          # Shared enumerations  
│   └── Models/         # TimeRange, FileMetadata, etc.
│
├── IIM.Infrastructure/  # External services, WSL, Storage
│   └── Platform/       # ✅ PHASE 1 COMPLETE
│       ├── IWslManager.cs (interface with InstallTorAndApplyProxyAsync)
│       ├── WslManager.cs (monolithic implementation - needs refactoring)
│       └── WslServiceOrchestrator.cs
│
├── IIM.Plugin.SDK/     # Plugin interfaces
│
├── IIM.Core/           # Domain models, business logic
│   ├── Models/         # Case, Evidence, Investigation, Inference models
│   ├── Services/       # Core business services (many partially implemented)
│   ├── AI/            
│   │   ├── ModelOrchestrator.cs (⚠️ Partial - needs DirectML integration)
│   │   └── DefaultModelOrchestrator.cs (has InferAsync stub)
│   ├── Inference/     
│   │   └── InferencePipeline.cs (⚠️ NEXT PRIORITY - needs completion)
│   ├── RAG/           # ⚠️ Empty implementations
│   └── Mediator/      # ✅ Custom mediator (complete)
│
├── IIM.Application/    # Application services
│   ├── Commands/      # ✅ CQRS commands with handlers
│   │   ├── Wsl/       # Including new ConfigureProxyCommand
│   │   ├── Models/    # Load/Unload model commands
│   │   └── Investigation/
│   ├── Queries/       # ✅ Query handlers
│   ├── Behaviors/     # ✅ 7 Pipeline behaviors implemented
│   └── Services/      
│       ├── InferenceService.cs (⚠️ Needs DirectML implementation)
│       └── InvestigationService.cs (partial implementation)
│
├── IIM.Api/           # HTTP endpoints, Minimal APIs
│   └── Program.cs     # All endpoints defined here
│
├── IIM.Components/    # Blazor UI components
│   └── Components/
│       └── Settings/
│           └── ProxySetup.razor (✅ New Tor proxy UI)
│
└── IIM.Desktop/       # Windows Forms host
    └── Program.cs     # Mediator configuration, DI setup

/tests
├── IIM.Core.Tests/
│   ├── Infrastructure/
│   │   └── WslManagerTests.cs (exists but incomplete)
│   └── Mocks/
│       ├── MockWslManager.cs
│       └── MockModelOrchestrator.cs
└── IIM.Integration.Tests/
```

## ✅ Phase 1: WSL & Docker Foundation (COMPLETE)

### What Was Implemented:
1. **WslManager with core functionality**:
   - `GetStatusAsync()` - Check WSL status
   - `IsWslEnabled()` - Simple boolean check
   - `EnableWsl()` - Install WSL with admin/non-admin paths
   - `EnsureDistroAsync()` - Install Ubuntu/Kali distros
   - `StartIim()` - Start all IIM services
   - `StartServicesAsync()` - Start Docker containers
   - `GetNetworkInfoAsync()` - Get WSL network details
   - `HealthCheckAsync()` - Comprehensive health monitoring
   - `SyncFilesAsync()` - Windows to WSL file sync
   - `InstallDistroAsync()` - Install from tar.gz
   - **NEW**: `InstallTorAndApplyProxyAsync()` - Tor proxy setup

2. **Service Orchestration**:
   - Qdrant on port 6333 (vector database)
   - PostgreSQL on port 5432 (case management)
   - MinIO on ports 9000/9001 (evidence storage)
   - MCP server on port 3000 (Kali forensic tools)

3. **Non-admin support**:
   - Generates PowerShell scripts for IT
   - Desktop instructions for manual install
   - Fallback to portable mode

### Current Issues with Phase 1:
- ⚠️ **WslManager is monolithic** (3000+ lines in one file)
- ⚠️ **Models not properly separated** (inline classes in WslManager.cs)
- ⚠️ **Tests incomplete** (MockWslManager throws NotImplementedException)
- ⚠️ **No actual DirectML implementation** despite being primary GPU strategy

## 🎯 Phase 2: Inference Pipeline & DirectML (CURRENT PRIORITY)

### Goal: Complete the AI inference pipeline with DirectML support

### Key Components Needing Implementation:

#### 1. **InferencePipeline.cs** 
Current state: Has structure but incomplete implementation
```csharp
// Needs:
- DirectML device initialization
- Proper queue processing
- Batch inference support
- Memory management for large models
```

#### 2. **ModelOrchestrator.cs**
Current state: Stub implementation, missing actual model loading
```csharp
// Needs:
- ONNX Runtime with DirectML provider
- Model download from member portal
- Model caching and versioning
- GPU memory management
```

#### 3. **InferenceService.cs**
Current state: Interface defined, implementation incomplete
```csharp
// Needs:
- Integration with InferencePipeline
- Support for all model types (LLM, Whisper, CLIP)
- Streaming responses for real-time transcription
```

### DirectML Integration Requirements:
1. **Device Enumeration**: Detect AMD GPU capabilities
2. **Model Optimization**: Convert models to ONNX format
3. **Memory Management**: Handle 128GB RAM + GPU VRAM
4. **Fallback Strategy**: CPU inference when GPU unavailable

## 📋 Implementation Priorities

### Sprint 1: DirectML Foundation (Week 1)
```csharp
// 1. Create DirectML device manager
public class DirectMLDeviceManager
{
    - EnumerateDevices()
    - CreateDevice(int deviceId)
    - GetDeviceCapabilities()
    - EstimateMemoryRequirements(model)
}

// 2. Integrate with ModelOrchestrator
- Add DirectML execution provider to ONNX Runtime
- Implement model warmup/optimization
- Add performance monitoring
```

### Sprint 2: Complete InferencePipeline (Week 2)
```csharp
// 1. Fix queue processing
- Implement priority-based scheduling
- Add batch processing for efficiency
- Handle cancellation properly

// 2. Add model-specific handlers
- LLM streaming with LlamaSharp
- Whisper with chunk processing
- CLIP with batch image encoding
```

### Sprint 3: RAG Implementation (Week 3)
```csharp
// 1. Document ingestion pipeline
- PDF/DOCX parsing with proper chunking
- Embedding generation via ONNX
- Qdrant integration for vector storage

// 2. Retrieval and generation
- Semantic search with reranking
- Context window management
- Citation extraction and formatting
```

### Sprint 4: Integration & Testing (Week 4)
```csharp
// 1. Wire up Mediator commands
- ProcessInvestigationCommand → InferencePipeline
- LoadModelCommand → ModelOrchestrator
- SearchDocumentsQuery → RAG pipeline

// 2. End-to-end testing
- Full investigation workflow
- Performance benchmarks
- Memory leak detection
```

## 🔧 Refactoring Needs

### Immediate (Before Phase 2):
1. **Break up WslManager.cs**:
   - WslManager.Core.cs (main logic)
   - WslManager.Services.cs (Docker management)
   - WslManager.Network.cs (networking)
   - WslManager.Proxy.cs (Tor/proxy setup)
   - Move models to separate files

2. **Fix Mock implementations**:
   - Complete MockWslManager
   - Add MockInferencePipeline
   - Enable unit testing

### During Phase 2:
1. **Standardize error handling** with Result<T> pattern
2. **Add telemetry** for performance monitoring
3. **Implement caching** for model outputs
4. **Add progress reporting** for long operations

## 🧪 Testing Requirements

### Unit Tests Needed:
- [ ] DirectMLDeviceManager device enumeration
- [ ] InferencePipeline queue management
- [ ] ModelOrchestrator load/unload cycles
- [ ] RAG chunking strategies
- [ ] Mediator command handlers

### Integration Tests:
- [ ] WSL + Docker service startup
- [ ] Model download and caching
- [ ] End-to-end inference pipeline
- [ ] Multi-modal investigation workflow

## 📊 Success Metrics

Phase 2 is complete when:
- ✅ DirectML successfully initialized on AMD GPU
- ✅ At least one model of each type loads (LLM, Whisper, CLIP)
- ✅ Inference pipeline processes requests with <500ms overhead
- ✅ RAG returns relevant results with citations
- ✅ Memory usage stays under 64GB for typical workload
- ✅ All mock implementations replaced
- ✅ 80% test coverage on critical paths

## 💡 Technical Decisions

1. **DirectML over ROCm**: ROCm unstable on Windows, DirectML is Microsoft-supported
2. **ONNX as primary format**: Best DirectML compatibility
3. **LlamaSharp for LLMs**: Native C# implementation, good Vulkan support
4. **Custom Mediator**: Avoid external dependencies, full control
5. **Monolithic to start**: Get working, then refactor (tech debt acknowledged)

## 🚀 Quick Start for Phase 2

```csharp
// 1. Initialize DirectML
var deviceManager = new DirectMLDeviceManager(logger);
var device = await deviceManager.CreateDevice(0); // Primary GPU

// 2. Load model with DirectML
var orchestrator = new ModelOrchestrator(logger, deviceManager);
await orchestrator.LoadModelAsync(new ModelRequest 
{
    ModelId = "llama-3.2-3b-onnx",
    Provider = "DirectML",
    DeviceId = 0
});

// 3. Run inference
var pipeline = new InferencePipeline(logger, orchestrator);
var result = await pipeline.ExecuteAsync<string>(new InferencePipelineRequest
{
    ModelId = "llama-3.2-3b-onnx",
    Input = "Analyze this evidence...",
    Priority = 2 // High priority
});
```

## 📝 Notes for Developer

- **Current state**: WSL management works, AI inference is mostly stubs
- **Next priority**: Get DirectML working with real models
- **Blockers**: No actual model files yet (need portal API integration)
- **Tech debt**: WslManager needs refactoring but works
- **Testing**: Mocks exist but many throw NotImplementedException

Remember: This is for a law enforcement training course - reliability and ease of use are paramount. The students are not developers, so the system must "just work" out of the box.